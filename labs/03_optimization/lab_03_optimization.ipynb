{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the minimum of a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local and global minimums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common problems in numerical analysis is to find the minimum of a multivariate function. To be precise, we are often mostly concerned with finding **a** minimum, not **the** minimum:\n",
    "\n",
    "- **Global minimum**: a point in the domain of a function such that the function is smaller at that point than anywhere else in the domain:\n",
    "\n",
    "$$\n",
    "\\bm{x} \\text{ is a global minimum of } f(\\bm{x})\n",
    "\\Leftrightarrow\n",
    "\\forall \\bm{x}' \\in \\mathbb{R}^{n} \\Rightarrow f(\\bm{x}') \\ge f(\\bm{x})\n",
    "$$\n",
    "\n",
    "But try to always read Mathematics in more intuitive terms, like if you are trying to explain the concepts to yourself. In this case, it reads: \n",
    "\n",
    "<center>\n",
    "\"A point is a global minimum if it is the lowest point of the function, end of story. Of all possible pits in the function, it is the lowest.\"\n",
    "</center>\n",
    "\n",
    "- **Local minima**: points in the domain of a function such that there is a radius (usually really small) around it where the function only goes up. In math notation it looks like this:\n",
    "\n",
    "$$\n",
    "\\bm{x} = (x_1, x_2, \\cdots, x_n) \\in \\mathbb{R}^{n} \\text{ is a local minimum of } f(\\bm{x})\n",
    "\\Leftrightarrow\n",
    "\\exists \\delta > 0 : \\forall \\bm{x}' \\in \\mathbb{R}^{n}, \\|\\bm{x}' - \\bm{x}\\| < \\delta \\Rightarrow f(\\bm{x}') \\ge f(\\bm{x})\n",
    "$$\n",
    "\n",
    "And in intuitive terms:\n",
    "\n",
    "<center>\n",
    "\"A local minimum is the bottom of a pit. The function may have many pits, each bottom is called a local minimum.\"\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "<img src=\"./minimum.png\" width=30%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding a local minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are several ways to look for a local minimum, which can be grouped as:\n",
    "\n",
    "- Trial and error\n",
    "- Analytic solutions\n",
    "- Numerical optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial and error\n",
    "\n",
    "Just try several values of $\\bm{x}$ to see which one has the smallest $f(\\bm{x})$. No guarantees of finding a local minimum really, let alone it being a global minimum. But this is a real approach: systematic forms of this trial-and-error search exist with different names. For example: genetic search, particle swarm optimization, tabu search, simulated annealing, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do some exercises. (Exercises with no stars are easy. Exercises with $\\star$ are not trivial but not too difficult. Exercises with $\\star\\star$ are harder. Exercises with $\\star\\star\\star$ are cruel.)\n",
    "\n",
    "**Exercise**: Consider the univariate (*i.e.* $\\bm{x} \\in \\mathbb{R}$ is just a variable, not a list of them) function $f(x) = 2x^2 - 3x + 5$. \n",
    "\n",
    "1. Plot the function and find the minimum visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. $\\star$ Write a program to test $100$ random guesses for $x$ in the interval $[-5, 5]$ and compare with your visual guess for the minimum. Then test $10000$ random guesses and see if the precision increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like every $10 \\times$ points we gain only one digit in accuracy... this method is really inefficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $f(x, y) = \\sin(y) \\exp\\left((1 - \\cos(x))^2\\right) + \\cos(x) \\exp\\left((1 - \\sin(y))^2\\right) + (x - y)^2$ is quite curious, with many local minima, and it's called \"Mishra's bird function\". Here is a plot of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def mishra_bird(x, y):\n",
    "    return \\\n",
    "        np.sin(y) * np.exp((1 - np.cos(x))**2) \\\n",
    "        + np.cos(x) * np.exp((1 - np.sin(y))**2) \\\n",
    "        + (x - y)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 1000)\n",
    "y = np.linspace(-10, 10, 1000)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "Z = mishra_bird(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cmap = 'hsv'\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X, Y, Z, cmap=cmap)\n",
    "ax.contourf(X, Y, Z, zdir='z', offset=-200, cmap=cmap, alpha=0.3)\n",
    "ax.contour(X, Y, Z, zdir='z', offset=-200, cmap=cmap)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.set_zlabel('$z$')\n",
    "ax.set_title('Mishra\\'s Bird Function')\n",
    "ax.set_xlim(-10, 10)\n",
    "ax.set_ylim(-10, 10)\n",
    "ax.set_zlim(-200, 400)\n",
    "ax.view_init(15, 250)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many local minima do you see in this $(-10, 10)^{2}$ domain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "1. Suppose you have a function $f: [0, 1] \\to \\mathbb{R}$, that is, a function $f(x)$ of one variable and restricted to the domain $[0, 1]$. You want to find the **global** minimum of the function with precision $0.1$ in $x$, so you build a regular *grid* of points in the interval $[0, 1]$ and test each one. How many points you have to test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. $\\star$ Suppose you have a function $f:[0, 1]^{n} \\to \\mathbb{R}$, that is, a function $f(\\bm{x}_1, \\bm{x}_2, \\cdots, \\bm{x}_n)$ of $n$ variables, all restricted to the domain $[0, 1]$. You want to find the **global** minimum of the function with precision $0.1$ in each variable $\\bm{x}_i$ by building a regular grid in the hypercube $[0, 1]^{n}$. How many points you have to test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. $\\star\\star$ Although you achieve precision of $0.1$ in each coordinate in the previous exercise, you may still be very, very far from the minimum point! Why is that? Hint:\n",
    "\n",
    "- Think of a line segment of length $0.1$, and the distance from the midpoint to the endpoints.\n",
    "- Now think of a square of edge length $0.1$ and the distance from the midpoint to the corners.\n",
    "- Now consider a cube of edge length $0.1$ and imagine the distance from the midpoint to the corners.\n",
    "- Then make a formula for the distance of the midpoint to one of the corners in the general case of the hypercube of edge length $0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. $\\star\\star\\star$ The volume of a hypercube is computed the same way as the area of a square or the volume of a regular cube, it is just the edge length raised to the power of the dimension. Consider a hypercube of dimension $n$ and edge length one. How much of the volume of the hypercube is within a distance $\\delta < 0.05$ from the walls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\star\\star\\star$ **Exercise** The Mishra's bird function above does not change too fast apparently. Maybe we can find the global minimum with good precision using a smart approach: look for the global minimum in a grid of resolution $0.1$, then build a finer grid only around the candidate for global minimum. Find the minimum of the Mishra's bird function in the domain $[-10, 10]$ with resolution $0.001$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving the value of the minimum using calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the formula for $f(\\bm{x})$ is known, and simple enough, one could try to find analytically the minima of it.\n",
    "\n",
    "First, take the partial derivative of $f(\\bm{x})$ with respect to each component of $\\bm{x}$. The tuple of all partial derivatives is the *gradient* of $f(\\bm{x})$:\n",
    "\n",
    "$$\n",
    "\\nabla f(\\bm{x}) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_{1}}(\\bm{x}) \\\\[10pt]\n",
    "\\frac{\\partial f}{\\partial x_{2}}(\\bm{x}) \\\\[10pt]\n",
    "\\vdots \\\\[10pt]\n",
    "\\frac{\\partial f}{\\partial x_{n}}(\\bm{x})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then set the gradient to the null vector $\\overrightarrow{\\bm{0}}$ and solve for $\\bm{x}$:\n",
    "\n",
    "$$\n",
    "\\nabla f(\\bm{x}) = \\overrightarrow{\\bm{0}} \n",
    "\\Leftrightarrow \\left\\{\n",
    "\\begin{matrix}\n",
    "\\frac{\\partial f}{\\partial x_{1}}(\\bm{x}) = 0 \\\\[10pt]\n",
    "\\frac{\\partial f}{\\partial x_{2}}(\\bm{x}) = 0 \\\\[10pt]\n",
    "\\vdots \\\\[10pt]\n",
    "\\frac{\\partial f}{\\partial x_{n}}(\\bm{x}) = 0\n",
    "\\end{matrix}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Solutions (yes, there may be more than one) to this system of equations will give you the \"critical points\" or \"stationary points\". It could be a minimum point, a maximum point, or \"saddle point\". In order to decide the nature of the solution, you need to consult the second derivative of $f(\\bm{x})$. For a multivariate function, the first derivative is a vector (the gradient) and the second derivative is a *matrix*, called the *Hessian* matrix:\n",
    "\n",
    "$$\n",
    "\\bm{H}_{f}(\\bm{x}) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^{2} f}{\\partial x_{1}^{2}}(\\bm{x}) &\n",
    "\\frac{\\partial^{2} f}{\\partial x_{1} \\partial x_{2}}(\\bm{x}) &\n",
    "\\cdots &\n",
    "\\frac{\\partial^{2} f}{\\partial x_{1} \\partial x_{n}}(\\bm{x})\\\\[10pt]\n",
    "\\frac{\\partial^{2} f}{\\partial x_{2} \\partial x_{1}}(\\bm{x}) &\n",
    "\\frac{\\partial^{2} f}{\\partial x_{2}^{2}}(\\bm{x}) &\n",
    "\\cdots &\n",
    "\\frac{\\partial^{2} f}{\\partial x_{2} \\partial x_{n}}(\\bm{x})\\\\[10pt]\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\[10pt]\n",
    "\\frac{\\partial^{2} f}{\\partial x_{n} \\partial x_{1}}(\\bm{x}) &\n",
    "\\frac{\\partial^{2} f}{\\partial x_{n} \\partial x_{2}}(\\bm{x}) &\n",
    "\\cdots &\n",
    "\\frac{\\partial^{2} f}{\\partial x_{n}^{2}}(\\bm{x})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note that the Hessian matrix is symmetric, because \n",
    "$\\frac{\\partial^{2} f}{\\partial x_{i} \\partial x_{j}}(\\bm{x}) = \n",
    "\\frac{\\partial^{2} f}{\\partial x_{j} \\partial x_{i}}(\\bm{x})$ (Clairaut's theorem).\n",
    "\n",
    "Now take each one of your critical points $\\bm{x}_{c}$ and compute the Hessian on it. And compute the eigenvalues $\\lambda_{i}$, $i \\in \\{1, 2, \\cdots, n\\}$ of the Hessian $\\bm{H}_{f}(\\bm{x}_c)$. And finally check:\n",
    "\n",
    "- If all eigenvalues are positive, $\\bm{x}_{c}$ is a minimum.\n",
    "- If all eigenvalues are negative, $\\bm{x}_{c}$ is a maximum.\n",
    "- If some eigenvalues are zero, and out of the remaining some are positive and some negative, then $\\bm{x}_{c}$ is a saddle point.\n",
    "\n",
    "This is all very complicated! But if this is useful, we must take a deep breath and face it...\n",
    "\n",
    "So here is how the analytic derivative method relates to machine learning:\n",
    "\n",
    "- Very few machine learning methods use the analytic derivative to find the optimal parameters of the model. The only one of which I'm aware is the *linear regression*.\n",
    "\n",
    "- Some methods use the analytically derived gradient but do not solve the system of equations to find the minima analytically. Instead, they use an iterative method to start from a random estimate for the minimum (a point chosen at random) and update this estimate successively using the gradient: change the estimate by adding a fraction of the *negative* of the gradient to it, repeat until convergence. This is the *gradient descent* method, to be discussed in the next section.\n",
    "\n",
    "- There is a way to obtain the numerical value of the gradient without having to determine the *formula* for the gradient, using a technique called ***automatic differentiation*** (or *autograd* for short). *This is the heart of ALL deep learning*, and there are several libraries to perform *autograd* efficiently and using advanced hardware, like GPUs or TPUs. For example: Tensorflow, Pytorch, Jax. The numerical gradient is then used with the *gradient descent* method to find a local minimum of the *loss function* (more on that later) with respect to the tunable parameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several methods for finding the numerical minimum of a function. One could have multiple courses on this topic, it is huge! Here is a book on it: Nocedal, J., & Wright, S. (n.d.). Numerical optimization (Second edition.). Springer-Verlag. 2006.\n",
    "\n",
    "So lets focus on the most useful method for numerical optimization in machine learning: the *gradient descent* method. The idea is simple: start at a point at random, and go walking in the direction of strongest decrease of the function until you reach the (local) minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, g, x0, eta, tol):\n",
    "    '''\n",
    "    f: function to minimize\n",
    "    g: gradient of f\n",
    "    x0: initial guess\n",
    "    eta: learning rate\n",
    "    tol: tolerance for stopping criterion\n",
    "    '''\n",
    "    x = x0\n",
    "    while True:\n",
    "        grad = g(x)\n",
    "        x = x - eta * np.array(grad)\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try this to find the minimum of $f(x) = 2x^2 - 3x + 5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 2.0*x**2 - 3.0*x + 5.0\n",
    "\n",
    "def grad_f(x):\n",
    "    return 4.0*x - 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A random number between -10 and 10.\n",
    "x_0 = -10.0 + 20.0 * np.random.rand()\n",
    "\n",
    "eta = 0.1\n",
    "\n",
    "tol = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min = gradient_descent(f, grad_f, x_0, eta, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Find a local minima of the mishra function starting from:\n",
    "\n",
    "- $\\bm{x}_{0} = (-3.0, -2.0)$\n",
    "- $\\bm{x}_{0} = (2.5, 5.0)$\n",
    "- $\\bm{x}_{0} = (-10.0, 10.0)$\n",
    "- $\\bm{x}_{0} = (10.0, -10.0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** The Rosenbrock function (https://en.wikipedia.org/wiki/Rosenbrock_function) is defined as:\n",
    "\n",
    "$$f(x,y)=(a-x)^{2}+b(y-x^{2})^{2}$$\n",
    "\n",
    "It has a global minimum at $(x,y) = (a,a^{2})$, where $f(x,y) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Show that the global minimum is indeed at $(x,y) = (a,a^{2})$:\n",
    "\n",
    "a) Using the gradient technique\n",
    "\n",
    "b) Not using the gradient technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, these parameters are set such that $a = 1$ and $b = 100$. It is a function made especially to make life hard for the *gradient descent*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x, y):\n",
    "    return (1 - x)**2 + 100 * (y - x**2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2, 2, 100)\n",
    "y = np.linspace(-1, 3, 100)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "Z = rosenbrock(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = 'viridis'\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X, Y, Z, cmap=cmap)\n",
    "ax.contourf(X, Y, Z, zdir='z', offset=-1000, cmap=cmap, alpha=0.3)\n",
    "ax.contour(X, Y, Z, zdir='z', offset=-1000, cmap=cmap)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.set_zlabel('$z$')\n",
    "ax.set_title('Rosenbrock function')\n",
    "ax.set_zlim(-1000, 2500)\n",
    "ax.view_init(20, -60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Find a local minima of the rosenbrock function starting from:\n",
    "\n",
    "- $\\bm{x}_{0} = (0.1, 0.0)$\n",
    "- $\\bm{x}_{0} = (1.0, 4.0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
